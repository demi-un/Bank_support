# rag_system.py 
import chromadb
from langchain_chroma import Chroma  
from langchain_huggingface import HuggingFaceEmbeddings  
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
import os
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

class SberSupportRAG:
    def __init__(self):
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        # 2. –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π ChromaDB
        self.client = chromadb.PersistentClient(path="./chroma_db_sber")
        
        # 3. –°–æ–∑–¥–∞–µ–º LangChain –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vectorstore = Chroma(
            client=self.client,
            collection_name="sber_support_knowledge",
            embedding_function=self.embeddings.embed_documents
        )
        
        # 4. –°–æ–∑–¥–∞–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )
        
        # 5. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç
        self.prompt_template = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –°–±–µ—Ä–±–∞–Ω–∫–∞.
        
–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É 8-800-555-00-00.

–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–í–æ–ø—Ä–æ—Å: {question}

–ü–æ–ª–µ–∑–Ω—ã–π, –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:"""
        
        self.PROMPT = PromptTemplate(
            template=self.prompt_template,
            input_variables=["context", "question"]
        )
        
        # 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å qwen2.5:3b
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å qwen2.5:3b...")
        self.llm = Ollama(
            model="qwen2.5:3b",
            temperature=0.3,
            num_predict=300
        )
        
        # 7. –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É RAG
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.retriever,
            chain_type_kwargs={"prompt": self.PROMPT},
            return_source_documents=True
        )
    
    def get_answer(self, question):
        """–ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        try:
            result = self.qa_chain({"query": question})
            return {
                "answer": result["result"],
                "sources": result.get("source_documents", [])
            }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            # Fallback
            docs = self.vectorstore.similarity_search(question, k=2)
            answer = "\n\n".join([doc.page_content for doc in docs])
            return {
                "answer": f"–í–æ—Ç —á—Ç–æ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:\n\n{answer}",
                "sources": docs
            }

# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã...")
    rag = SberSupportRAG()
    
    test_questions = [
        "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?",
        "–£ –º–µ–Ω—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç VPN, —á—Ç–æ –¥–µ–ª–∞—Ç—å?",
        "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–∏—Å—Ç–µ–º–µ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏?"
    ]
    
    for question in test_questions:
        print(f"\n{'='*60}")
        print(f"‚ùì –í–æ–ø—Ä–æ—Å: {question}")
        result = rag.get_answer(question)
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {result['answer'][:200]}...")
        if result['sources']:
            print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(result['sources'])}")